# Stochastic Gradient Descent in Over-Parameterized Learning

This repository contains our class project exploring the behavior of **Stochastic Gradient Descent (SGD)** in **over-parameterized machine learning models**. Our work includes a literature-based investigation and a set of supporting numerical experiments.

## ðŸ“„ Abstract

Many modern machine learning models are over-parameterized. While the classical theory for under-parameterized models is well developed, the over-parameterized regime remains largely unexplored. Interestingly, empirical findings reveal that such models often exhibit counterintuitive properties:  
- Local minima are frequently global,  
- SGD converges surprisingly fast, and  
- Classical notions of generalization behave differently.  

In this report, we investigate some of these phenomena and examine how over-parameterization enhances the behavior of SGD, both theoretically and empirically.

## Authors

This work was completed as part of a graduate course project for the course SIE 596 : Machine Learning Optimization

Team Members:
- **Jeffrey Mei** â€“ [jmei@math.arizona.edu](mailto:jmei@math.arizona.edu)  
- **Cody Melcher** â€“ [cmelcher@math.arizona.edu](mailto:cmelcher@math.arizona.edu)  
- **Kamaljeet Singh** â€“ [kamaljeetsingh@arizona.edu](mailto:kamaljeetsingh@arizona.edu)

