# Stochastic Gradient Descent in Over-Parameterized Learning

This repository contains our class project exploring the behavior of **Stochastic Gradient Descent (SGD)** in **over-parameterized machine learning models**. Our work includes a literature-based investigation and a set of supporting numerical experiments.

## üìÑ Abstract

Many modern machine learning models are over-parameterized. While the classical theory for under-parameterized models is well developed, the over-parameterized regime remains largely unexplored. Interestingly, empirical findings reveal that such models often exhibit counterintuitive properties:  
- Local minima are frequently global,  
- SGD converges surprisingly fast, and  
- Classical notions of generalization behave differently.  

In this report, we investigate some of these phenomena and examine how over-parameterization enhances the behavior of SGD, both theoretically and empirically.

## Authors

This work was completed as part of a graduate course project for the course SIE 596 : Machine Learning Optimization

Team Members:
- **Jeffrey Mei** ‚Äì [jmei@math.arizona.edu](mailto:jmei@math.arizona.edu)  
- **Cody Melcher** ‚Äì [cmelcher@math.arizona.edu](mailto:cmelcher@math.arizona.edu)  
- **Kamaljeet Singh** ‚Äì [kamaljeetsingh@arizona.edu](mailto:kamaljeetsingh@arizona.edu)


## üìÅ Contents

- `SGD_overparameterized.pdf` ‚Äì Final project report  
- `experiments/` ‚Äì (optional) Notebooks or scripts for empirical analysis  




