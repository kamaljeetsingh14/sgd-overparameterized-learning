# Stochastic Gradient Descent in Over-Parameterized Learning

This repository contains our class project exploring the behavior of **Stochastic Gradient Descent (SGD)** in **over-parameterized machine learning models**. Our work includes a literature-based investigation and a set of supporting numerical experiments.

## ğŸ“„ Abstract

Many modern machine learning models are over-parameterized. While the classical theory for under-parameterized models is well developed, the over-parameterized regime remains largely unexplored. Interestingly, empirical findings reveal that such models often exhibit counterintuitive properties:  
- Local minima are frequently global,  
- SGD converges surprisingly fast, and  
- Classical notions of generalization behave differently.  

In this report, we investigate some of these phenomena and examine how over-parameterization enhances the behavior of SGD, both theoretically and empirically.

## ğŸ‘¨â€ğŸ‘©â€ğŸ‘¦ Authors

This work was completed as part of a graduate course project in the Department of Mathematics at **The University of Arizona**.

- **Jeffrey Mei** â€“ [jmei@math.arizona.edu](mailto:jmei@math.arizona.edu)  
- **Cody Melcher** â€“ [cmelcher@math.arizona.edu](mailto:cmelcher@math.arizona.edu)  
- **Kamaljeet Singh** â€“ [kamaljeetsingh@arizona.edu](mailto:kamaljeetsingh@arizona.edu)

We gratefully acknowledge the collaborative effort of our team members and the guidance provided during the course.

## ğŸ“ Contents

- `SGD_overparameterized.pdf` â€“ Final project report  
- `experiments/` â€“ (optional) Notebooks or scripts for empirical analysis  




